{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5191ba01-86ca-4697-917f-4b53566ab90c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c13a0-9590-4303-aea8-87ac464665d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 10: Text data preprocerssing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd57fa59-c316-4b56-bd81-365d6d2f3e33",
   "metadata": {},
   "source": [
    "Notebook is generated with help of DeepSeek with prompt:\n",
    "```\n",
    "Give me example of Python code for Jupyter notebook to demonstrate NLP techniques for text preprocessing\n",
    "(1) `re` regexp library\n",
    "(2) `nltk` library\n",
    "(3) `spaCy` library\n",
    "Show how these libraries work with the same piece of text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397754cf-18af-4507-ab6d-32431318aef1",
   "metadata": {},
   "source": [
    "### 1. Libraries and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d397bbd-7456-44ff-b623-34e09962b2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4852e-e454-4300-9dec-0d7828bef3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5434b5-942d-419a-aab7-8895c83fccc9",
   "metadata": {},
   "source": [
    "### 2. Preprocessing demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c607bd-e146-45cf-854b-7a74c3592810",
   "metadata": {},
   "source": [
    "#### 2.1. Sample document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7699f96-1d2d-489e-9e7e-ada1b306ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Hello! This is a sample text for NLP preprocessing demonstration. \n",
    "It contains various elements: emails like john.doe@email.com, \n",
    "URLs like https://www.example.com, and phone numbers like (555) 123-4567.\n",
    "We'll also process some SPECIAL characters & numbers like 42! \n",
    "The quick brown foxes are jumping over 10 lazy dogs. \n",
    "Running, jumped, and runs will be stemmed/lemmatized.\n",
    "Let's test contractions: don't, can't, we're going to handle them!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946f4ff-cb56-41a4-b2d5-b1120d065912",
   "metadata": {},
   "source": [
    "#### 2.2. Regex library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac261df5-adbe-4f30-b374-a4c8b53a3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_regex(text):\n",
    "    \"\"\"\n",
    "    Uses of `re` library for text preprocessing.\n",
    "\n",
    "    Args:\n",
    "      :text: text to process\n",
    "\n",
    "    Returns:\n",
    "      processed text\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    \n",
    "    # 1. Convert to lowercase\n",
    "    text_lower = text.lower()\n",
    "    print(\"1. Lowercase:\")\n",
    "    print(text_lower)\n",
    "    \n",
    "    # 2. Remove URLs\n",
    "    text_no_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text_lower)\n",
    "    print(\"\\n2. Remove URLs:\")\n",
    "    print(text_no_urls)\n",
    "    \n",
    "    # 3. Remove email addresses\n",
    "    text_no_emails = re.sub(r'\\S+@\\S+', '', text_no_urls)\n",
    "    print(\"\\n3. Remove Emails:\")\n",
    "    print(text_no_emails)\n",
    "    \n",
    "    # 4. Remove phone numbers\n",
    "    text_no_phones = re.sub(r'\\(\\d{3}\\)\\s*\\d{3}-\\d{4}', '', text_no_emails)\n",
    "    print(\"\\n4. Remove Phone Numbers:\")\n",
    "    print(text_no_phones)\n",
    "    \n",
    "    # 5. Remove special characters and numbers (keep only letters and spaces)\n",
    "    text_clean = re.sub(r'[^a-zA-Z\\s]', ' ', text_no_phones)\n",
    "    print(\"\\n5. Remove Special Characters & Numbers:\")\n",
    "    print(text_clean)\n",
    "    \n",
    "    # 6. Remove extra whitespace\n",
    "    text_final = re.sub(r'\\s+', ' ', text_clean).strip()\n",
    "    print(\"\\n6. Remove Extra Whitespace:\")\n",
    "    print(text_final)\n",
    "    \n",
    "    # 7. Simple tokenization using regex\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text_final)\n",
    "    print(\"\\n7. Tokens:\")\n",
    "    print(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97d46e-2275-4889-af88-88155db20c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Text preprocessing with `re` library\")\n",
    "print(\"=\" * 60, '\\n')\n",
    "\n",
    "re_tokens = preprocess_with_regex(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcd757-762a-4c50-9df8-8918ee441042",
   "metadata": {},
   "source": [
    "#### 2.3. NLTL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac637c-ceea-48e7-bc8a-315d1e88f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_nltk(text):\n",
    "    \"\"\"\n",
    "    Uses of `NLTK` library for text preprocessing.\n",
    "\n",
    "    Args:\n",
    "      :text: text to process\n",
    "\n",
    "    Returns:\n",
    "      processed text\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    \n",
    "    # 1. Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"1. Sentence Tokenization:\")\n",
    "    for i, sent in enumerate(sentences, 1):\n",
    "        print(f\"   {i}. {sent}\")\n",
    "    \n",
    "    # 2. Word tokenization\n",
    "    words = word_tokenize(text)\n",
    "    print(f\"\\n2. Word Tokenization ({len(words)} tokens):\")\n",
    "    print(words[:15])  # Show first 15 tokens\n",
    "    \n",
    "    # 3. Convert to lowercase\n",
    "    words_lower = [word.lower() for word in words]\n",
    "    print(f\"\\n3. Lowercase Tokens:\")\n",
    "    print(words_lower[:15])\n",
    "    \n",
    "    # 4. Remove punctuation and numbers\n",
    "    words_alpha = [word for word in words_lower if word.isalpha()]\n",
    "    print(f\"\\n4. Alphabetical Tokens Only:\")\n",
    "    print(words_alpha[:15])\n",
    "    \n",
    "    # 5. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words_no_stopwords = [word for word in words_alpha if word not in stop_words]\n",
    "    print(f\"\\n5. Remove Stopwords:\")\n",
    "    print(f\"   Stopwords removed: {[word for word in words_alpha if word in stop_words]}\")\n",
    "    print(f\"   Remaining tokens: {words_no_stopwords}\")\n",
    "    \n",
    "    # 6. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words_stemmed = [stemmer.stem(word) for word in words_no_stopwords]\n",
    "    print(f\"\\n6. Stemming:\")\n",
    "    stem_examples = list(zip(words_no_stopwords[:10], words_stemmed[:10]))\n",
    "    for original, stemmed in stem_examples:\n",
    "        print(f\"   {original} -> {stemmed}\")\n",
    "    \n",
    "    # 7. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_no_stopwords]\n",
    "    print(f\"\\n7. Lemmatization:\")\n",
    "    lemma_examples = list(zip(words_no_stopwords[:10], words_lemmatized[:10]))\n",
    "    for original, lemma in lemma_examples:\n",
    "        print(f\"   {original} -> {lemma}\")\n",
    "    \n",
    "    # 8. Part-of-Speech Tagging\n",
    "    pos_tags = pos_tag(words_no_stopwords[:15])\n",
    "    print(f\"\\n8. POS Tagging (first 15 tokens):\")\n",
    "    for word, pos in pos_tags:\n",
    "        print(f\"   {word}: {pos}\")\n",
    "    \n",
    "    return words_no_stopwords, words_stemmed, words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446f2f3-4c20-4c9e-a76b-ac0770a8bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Text preprocessing with `NLTK` library\")\n",
    "print(\"=\" * 60, '\\n')\n",
    "\n",
    "nltk_tokens, nltk_stemmed, nltk_lemmatized = preprocess_with_nltk(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb98830-4d0c-416a-9a8c-57d8aee8c6d5",
   "metadata": {},
   "source": [
    "#### 2.4. SpaCy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b970ef0-aa29-4ce0-a334-16aeb667a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Uses of `spaCy` library for text preprocessing.\n",
    "\n",
    "    Args:\n",
    "      :text: text to process\n",
    "\n",
    "    Returns:\n",
    "      processed text\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Original Text:\")\n",
    "    print(text)\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # 1. Sentence segmentation\n",
    "    print(\"1. Sentence Segmentation:\")\n",
    "    for i, sent in enumerate(doc.sents, 1):\n",
    "        print(f\"   {i}. {sent.text}\")\n",
    "    \n",
    "    # 2. Tokenization with linguistic features\n",
    "    print(f\"\\n2. Detailed Token Analysis (first 20 tokens):\")\n",
    "    print(\n",
    "        f\"{'Token':<15} {'Lemma':<15} {'POS':<10}\",\n",
    "        f\"{'Tag':<10} {'Stopword':<10} {'Alpha':<8}\"\n",
    "    )\n",
    "    print(\"-\" * 80)\n",
    "    for token in doc[:20]:\n",
    "        print(\n",
    "            f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10}\",\n",
    "            f\"{token.tag_:<10} {token.is_stop:<10} {token.is_alpha:<8}\"\n",
    "        )\n",
    "    \n",
    "    # 3. Extract various components\n",
    "    # Clean tokens (no stopwords, punctuation, or spaces)\n",
    "    clean_tokens = [\n",
    "        token.lemma_.lower() for token in doc \n",
    "        if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n3. Clean Tokens (lemmatized, no stopwords/punctuation):\")\n",
    "    print(clean_tokens)\n",
    "    \n",
    "    # 4. Named Entity Recognition\n",
    "    print(f\"\\n4. Named Entity Recognition:\")\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    if entities:\n",
    "        for entity, label in entities:\n",
    "            print(f\"   {entity} -> {label}\")\n",
    "    else:\n",
    "        print(\"   No named entities detected in this text.\")\n",
    "    \n",
    "    # 5. Noun phrases\n",
    "    print(f\"\\n5. Noun Phrases:\")\n",
    "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    for i, phrase in enumerate(noun_phrases[:10], 1):\n",
    "        print(f\"   {i}. {phrase}\")\n",
    "    \n",
    "    # 6. Dependency parsing visualization (first sentence)\n",
    "    first_sent = next(doc.sents)\n",
    "    print(f\"\\n6. Dependency Parsing (first sentence - sample):\")\n",
    "    print(f\"   Sentence: {first_sent}\")\n",
    "    print(f\"   Root token: {first_sent.root.text} (POS: {first_sent.root.pos_})\")\n",
    "    print(\"   Sample dependencies:\")\n",
    "    for token in first_sent[:8]:  # Show first 8 tokens\n",
    "        print(f\"     {token.text:<10} -> {token.dep_:<12} -> {token.head.text}\")\n",
    "    \n",
    "    return clean_tokens, entities, noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc10c8-69db-4179-8a7d-537f36e61c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Text preprocessing with `spaCy` library\")\n",
    "print(\"=\" * 60, '\\n')\n",
    "\n",
    "spacy_tokens, spacy_entities, spacy_noun_phrases = preprocess_with_spacy(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccaea2-3d89-4e83-bdf3-176f1be712cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
