{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a518c2-9377-4fbf-b489-308e0fbcadc5",
   "metadata": {},
   "source": [
    "# Deep Learning for Business Applications course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f75f4-0c76-4b4f-afcb-42bb69f9f7ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOPIC 3: Computer Vision advanced. SotA model example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d5ce9-5cf8-45c6-8abd-c201e25764e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2229be-0b59-4a78-bba1-c48b63c788c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76303635-bd19-434f-97f9-f1a852bab803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3269f47-2673-4b0f-b7dc-5662d4cf6476",
   "metadata": {},
   "source": [
    "### 2. Looking ahead - ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be7ba6-1bdd-45e5-a2b1-4a653153f507",
   "metadata": {},
   "source": [
    "Let's look at the [Vision Transformer (ViT)](https://huggingface.co/google/vit-base-patch16-224) that is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183d4a9-9e66-41b7-82a1-a5d3913757b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/vit-base-patch16-224'\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a02851-590a-462a-b360-4273bb8410b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_plot_coco_images(\n",
    "    idxs, processor, model, \n",
    "    rows=None, cols=None, figsize=(12, 8)\n",
    "):\n",
    "    \"\"\"\n",
    "    Classify and plot multiple images in a grid layout with titles.\n",
    "    \n",
    "    Parameters:\n",
    "    - idxs: Indexes of images from COCO dataset\n",
    "    - processor: Image processor for the model\n",
    "    - model: Model to classify omages\n",
    "    - rows: Number of rows in the grid (optional - will be calculated if not provided)\n",
    "    - cols: Number of columns in the grid (optional - will be calculated if not provided)\n",
    "    - figsize: Tuple specifying the figure size (width, height)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_images = len(idxs)\n",
    "    \n",
    "    if rows is None and cols is None:\n",
    "        cols = int(np.ceil(np.sqrt(n_images)))\n",
    "        rows = int(np.ceil(n_images / cols))\n",
    "    elif rows is None:\n",
    "        rows = int(np.ceil(n_images / cols))\n",
    "    elif cols is None:\n",
    "        cols = int(np.ceil(n_images / rows))\n",
    "\n",
    "    # Use ViT model classifier to title an image\n",
    "    images, titles = [], []\n",
    "    for idx in idxs:\n",
    "        url = f'http://images.cocodataset.org/{idx}.jpg'\n",
    "        image = Image.open(requests.get(url, stream=True).raw)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # model predicts one of the 1000 ImageNet classes\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "        title = model.config.id2label[predicted_class_idx]\n",
    "        images.append(image)\n",
    "        titles.append(title)\n",
    "        print('Predicted class:', title)\n",
    "    \n",
    "    # Create default titles if not provided\n",
    "    if titles is None:\n",
    "        titles = [f'Image {i+1}' for i in range(n_images)]\n",
    "    elif len(titles) != n_images:\n",
    "        # Extend or truncate titles to match number of images\n",
    "        titles = titles[:n_images] + [f'Image {i+1}' for i in range(len(titles), n_images)]\n",
    "    \n",
    "    # Create the figure and subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    \n",
    "    # If there's only one row or column, axes might not be a 2D array\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Flatten the axes array for easier indexing\n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    # Plot each image\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        ax = axes_flat[i]\n",
    "        \n",
    "        # Handle different image types\n",
    "        if isinstance(img, str):  # File path\n",
    "            img_data = plt.imread(img)\n",
    "            ax.imshow(img_data)\n",
    "        elif isinstance(img, Image.Image):  # PIL Image\n",
    "            ax.imshow(np.array(img))\n",
    "        else:  # Assume it's a numpy array or compatible\n",
    "            ax.imshow(img)\n",
    "        \n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')  # Hide axes\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(n_images, len(axes_flat)):\n",
    "        axes_flat[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e78b24-74c9-4063-b414-ba5f070cf87a",
   "metadata": {},
   "source": [
    "### 3. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b7887-9058-4404-b526-14a43de91d47",
   "metadata": {},
   "source": [
    "You will see that it may give errors with a hard cases with many objects at the picture. That is why will move to `Object detection` problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d43fcc-0c7a-4d17-9d8b-9ee2e5c04024",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [\n",
    "    'test-stuff2017/000000027294',\n",
    "    'test-stuff2017/000000027303',\n",
    "    'test-stuff2017/000000027155',\n",
    "    'test-stuff2017/000000027049',\n",
    "    'test-stuff2017/000000027057',\n",
    "    'test-stuff2017/000000027833',\n",
    "    'test-stuff2017/000000022859',\n",
    "    'test-stuff2017/000000022895',\n",
    "    'test-stuff2017/000000023209'\n",
    "]\n",
    "\n",
    "classify_and_plot_coco_images(\n",
    "    idxs=idxs, \n",
    "    processor=processor, \n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fef8f-bd82-4709-a605-6a543d84bcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
